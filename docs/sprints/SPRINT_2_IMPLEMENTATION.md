# Sprint 2: Pipeline Architecture Patterns - Implementation Log

**Status**: Phase 2 Complete âœ… | Phase 3 Next ðŸ”„
**Date Started**: 2026-01-21
**Last Updated**: 2026-01-21
**Objective**: Refactor ingestion pipeline with proper design patterns and integrate complete database layer with Supabase

---

## Critical Issues Addressed

### ðŸ”´ Fixed Issues

1. **Schema Mismatch** - Orchestrator referenced `doc.file_path`, `doc.status_details`, `doc.chunks` that didn't exist
2. **Thread Safety** - Global `_embeddings_cache` in `ingestion/chunking.py:54` was not thread-safe
3. **Hardcoded Parsers** - No extensibility for adding new document types
4. **Linear Pipeline** - Sequential if-statements instead of DAG-based architecture

---

## Phase 1: Code Refactoring âœ… COMPLETE

**Goal**: Refactor all code patterns without touching the database

### Task 1.1: Exception Hierarchy âœ…

**File**: [core/exceptions.py](../../core/exceptions.py)

**What Changed**:
- Created central exception hierarchy
- Base exception: `MeditationDBError`
- Organized exceptions by category: Configuration, Pipeline, Database
- All modules now import from this central location

**Exception Tree**:
```
MeditationDBError (base)
â”œâ”€â”€ ConfigurationError
â”œâ”€â”€ PipelineError
â”‚   â”œâ”€â”€ ParsingError
â”‚   â”œâ”€â”€ ChunkingError
â”‚   â””â”€â”€ EmbeddingError
â””â”€â”€ DatabaseError
    â”œâ”€â”€ DocumentNotFoundError
    â””â”€â”€ SchemaValidationError
```

**Impact**: Consistent error handling across entire application

---

### Task 1.2: Core Interfaces âœ…

**File**: [core/interfaces.py](../../core/interfaces.py)

**What Changed**:
- Created `Parser` protocol for structural typing
- Created `ParseResult` dataclass for parser outputs
- Created `PipelineStage` ABC for pipeline stages
- Created `PipelineContext` dataclass with immutable update pattern
- Created `StageStatus` enum for stage execution tracking

**Key Features**:
- **Immutable Context**: `with_update()` method creates new instances instead of mutating
- **Protocol-based Design**: Duck typing with type safety
- **Dependency Declaration**: Stages declare `required_stages`

---

### Task 1.3: Parser Refactoring âœ…

**File**: [ingestion/parsing.py](../../ingestion/parsing.py)

**What Changed**:
- Refactored from 2 standalone functions to Strategy pattern
- Created `URLParser` class for HTTP/HTTPS URLs
- Created `PDFParser` class for PDF files
- Created `ParserFactory` for automatic parser selection
- Added backward-compatible deprecated functions with warnings

**Old API (deprecated)**:
```python
content = html_to_markdown("https://example.com")  # Shows deprecation warning
```

**New API**:
```python
factory = ParserFactory()
result = factory.parse("https://example.com")
markdown = result.content
title = result.title
```

**Benefits**:
- Easy to add new parsers (just implement `Parser` protocol)
- Automatic parser selection based on source type
- Rich metadata in `ParseResult`
- Proper error handling with `ParsingError`

---

### Task 1.4: Thread-Safe Embeddings Cache âœ… CRITICAL FIX

**File**: [ingestion/chunking.py](../../ingestion/chunking.py)

**What Changed**:
- Replaced global class variable `_embeddings_cache` with `ThreadSafeEmbeddingsCache` singleton
- Implemented double-checked locking pattern
- Each model has its own lock (concurrent loading of different models)
- Read operations don't require locks (after initial load)

**Before (UNSAFE)**:
```python
class MarkdownChunker:
    _embeddings_cache: Dict[str, Any] = {}  # NOT thread-safe!

    def _get_embeddings(self):
        if model not in self._embeddings_cache:
            self._embeddings_cache[model] = HuggingFaceEmbeddings(model)
        return self._embeddings_cache[model]
```

**After (SAFE)**:
```python
class ThreadSafeEmbeddingsCache:
    _instance = None
    _instance_lock = threading.Lock()

    def get_embeddings(self, model_name: str):
        # Fast path: check cache without lock
        if model_name in self._cache:
            return self._cache[model_name]

        # Slow path: acquire model-specific lock
        with self._get_model_lock(model_name):
            if model_name in self._cache:  # Double-check
                return self._cache[model_name]
            # Load and cache
            self._cache[model_name] = HuggingFaceEmbeddings(model_name)
            return self._cache[model_name]
```

**Impact**:
- âœ… No more race conditions with parallel processing
- âœ… Thread-safe for `ThreadPoolExecutor` in chunking
- âœ… Model-specific locks allow concurrent loading of different models

---

### Task 1.5: Pipeline Stages âœ…

**File**: [ingestion/stages.py](../../ingestion/stages.py)

**What Changed**:
- Created `ParsingStage` - parses source documents
- Created `ChunkingStage` - chunks parsed markdown
- Created `EmbeddingStage` - embeds chunks and stores in vector DB
- Each stage declares dependencies and implements `execute()`

**Stage Dependencies**:
```
ParsingStage (no dependencies)
    â†“
ChunkingStage (requires: parsing)
    â†“
EmbeddingStage (requires: chunking)
```

**Stage Interface**:
```python
class ParsingStage(PipelineStage):
    @property
    def name(self) -> str:
        return "parsing"

    @property
    def required_stages(self) -> List[str]:
        return []  # No dependencies

    async def execute(self, context: PipelineContext) -> PipelineContext:
        # Parse source and return updated context
```

---

### Task 1.6: DAG Orchestrator âœ…

**File**: [ingestion/orchestrator.py](../../ingestion/orchestrator.py)

**What Changed**:
- Created `PipelineOrchestrator` with DAG-based execution
- Implements topological sort for stage ordering
- Validates DAG (no cycles)
- Automatic dependency resolution
- Refactored `IngestionOrchestrator` to use DAG pipeline internally
- Maintains backward-compatible API

**Old Architecture** (Linear):
```python
if doc.status == PENDING:
    parse(doc)
if doc.status == PARSED:
    chunk(doc)
if doc.status == CHUNKED:
    embed(doc)
```

**New Architecture** (DAG):
```python
stages = [ParsingStage(...), ChunkingStage(...), EmbeddingStage(...)]
pipeline = PipelineOrchestrator(stages)
context = await pipeline.execute(PipelineContext(source=...))
```

**Features**:
- âœ… Topological sort for correct execution order
- âœ… Cycle detection prevents infinite loops
- âœ… Skip already-completed stages
- âœ… Continue on partial failures
- âœ… Clear dependency tracking

---

### Task 1.7: Package Exports âœ…

**File**: [ingestion/__init__.py](../../ingestion/__init__.py)

**What Changed**:
- Added new API exports while maintaining backward compatibility
- Organized exports into "Backward Compatible" and "New API" sections
- Comprehensive `__all__` list

**New Exports**:
```python
# New API
from ingestion import (
    ParserFactory, URLParser, PDFParser,
    ThreadSafeEmbeddingsCache,
    PipelineOrchestrator,
    ParsingStage, ChunkingStage, EmbeddingStage
)

# Old API still works
from ingestion import (
    MarkdownChunker, VectorStoreManager,
    IngestionOrchestrator,
    html_to_markdown, parse_pdf  # Deprecated
)
```

---

### Phase 1 Verification âœ…

**Tests Run**:
```bash
âœ“ Exception hierarchy imports successfully
âœ“ Core interfaces import successfully
âœ“ Thread-safe cache singleton working
âœ“ Pipeline context immutability working
```

**Known Issue**:
- `psycopg2` dependency requires PostgreSQL client libraries
- Not related to Sprint 2 refactoring
- Can be resolved with `brew install postgresql` or `poetry add psycopg2-binary`

---

## Phase 2: Database Setup âœ… COMPLETE

**Goal**: Create complete schema with all necessary fields and integrate with pipeline

### Task 2.1: Update Schema Definition âœ…

**File**: [db/schema.py](../../db/schema.py)

**What Changed**:
- Added 3 missing fields to Document model:
  - `file_path` - Source URL or file path (for pipeline resume)
  - `status_details` - Error messages and debugging info
  - `chunks` - Temporary JSONB storage before embedding
- Added index on `status` column for query optimization
- Added relationship to Chunk model with cascade delete
- **New**: Complete `Chunk` model with 7 fields:
  - `id`, `uuid`, `document_id`, `chunk_text`, `chunk_index`, `chunk_metadata`, `created_at`
  - Links to LangChain embeddings via `uuid`
  - Foreign key to Document with cascading delete

**Architecture Principle**:
- Application schema (documents, chunks) = metadata and status tracking
- LangChain schema (langchain_pg_embedding) = vectors and embeddings
- Link via: `chunks.uuid` â†” `langchain_pg_embedding.uuid`

---

### Task 2.2: Update CRUD Operations âœ…

**File**: [db/crud.py](../../db/crud.py)

**What Changed**:
- Removed `EmbeddingCRUD` class (referenced non-existent schema.Embedding)
- Enhanced `DocumentCRUD` with 7 new methods:
  - `create_document()` - Now requires `file_path` parameter
  - `update_status()` - Update pipeline status with details
  - `update_markdown()` - Save parsed content
  - `store_chunks()` - Temporary JSONB storage
  - `clear_chunks()` - Free up space after embedding
  - `get_documents_by_status()` - Filter by status
  - `get_failed_documents()` - Get all FAILED documents
- **New**: `ChunkCRUD` class with 3 methods:
  - `create_chunks_batch()` - Bulk insert chunks
  - `get_chunks_by_document()` - Get all chunks for a document
  - `get_chunk_by_uuid()` - Link vector search results to chunks

---

### Task 2.3: Configuration Updates âœ…

**Files**: [config/settings.py](../../config/settings.py), [db/database.py](../../db/database.py)

**What Changed**:

**DatabaseSettings enhancements**:
- Added `pool_timeout` (default: 30s)
- Added `pool_recycle` (default: 3600s / 1 hour)
- Added `is_supabase` property for environment detection
- Optimized defaults for Supabase (pool_size=10, max_overflow=20)

**Database connection enhancements**:
- Auto-detects Supabase from URL (`is_supabase` property)
- Supabase-optimized pooling:
  - `pool_pre_ping=True` - Verify connections before use
  - `pool_recycle=3600` - Prevent stale connections
- Enhanced `init_db()`:
  - Better logging with âœ“ checkmarks
  - Verifies pgvector extension
  - Lists created tables
  - Better error handling

---

### Task 2.4: Pipeline Integration âœ…

**File**: [ingestion/stages.py](../../ingestion/stages.py)

**What Changed**:

**EmbeddingStage enhancement**:
- Now adds UUIDs to chunks BEFORE embedding
- Metadata enrichment:
  - `uuid` - Generated UUID for database linking
  - `original_doc_id` - Parent document ID
  - `original_doc_title` - Parent document title
  - `chunk_index` - Position in document

**New: DatabasePersistenceStage (Stage 4)**:
- Saves chunk metadata to database after embedding
- Updates document status to COMPLETED
- Clears temporary chunk storage
- Full error handling with DatabaseError

**Pipeline Flow**:
```
ParsingStage â†’ ChunkingStage â†’ EmbeddingStage â†’ DatabasePersistenceStage
```

---

### Task 2.5: Orchestrator Updates âœ…

**File**: [ingestion/orchestrator.py](../../ingestion/orchestrator.py)

**What Changed**:
- Creates Document record at pipeline start (with PENDING status)
- Passes `document_id` through entire pipeline context
- Integrates DatabasePersistenceStage as 4th stage
- Handles FAILED status updates on pipeline errors
- Supports resume by document ID:
  ```python
  # Resume existing document
  orchestrator.process(document_id=123)

  # Or create new
  orchestrator.process(source="https://example.com", title="My Doc")
  ```

---

### Task 2.6: Verification Script âœ…

**File**: [scripts/verify_database.py](../../scripts/verify_database.py)

**What It Does**:
Comprehensive 7-check verification:
1. Database connection test
2. pgvector extension check
3. Tables exist (documents, chunks)
4. Document schema (12 columns)
5. Chunk schema (7 columns)
6. CRUD operations test
7. Configuration validation

**Usage**:
```bash
poetry run python scripts/verify_database.py
```

Expected output: "âœ“ ALL CHECKS PASSED"

---

## Phase 3: Deployment & Testing (Next)

**Goal**: Deploy to Supabase and verify end-to-end functionality

### Task 3.1: Alembic Setup (Pending)

**Commands**:
```bash
# Initialize Alembic
poetry run alembic init alembic

# Configure alembic/env.py
# Generate initial migration
poetry run alembic revision --autogenerate -m "Initial schema: documents and chunks"

# Review migration file
# Apply to Supabase
poetry run alembic upgrade head
```

**Key Files**:
- `alembic.ini` - Alembic configuration
- `alembic/env.py` - Migration environment (needs configuration)
- `alembic/versions/XXXX_initial_schema.py` - Generated migration

---

### Task 3.2: Supabase Configuration (Pending)

**Steps**:
1. Create Supabase project (or use existing)
2. Get Transaction mode connection string (port 6543)
3. Update `.env`:
   ```bash
   DB_URL="postgresql://postgres.PROJECT_REF:PASSWORD@aws-0-REGION.pooler.supabase.com:6543/postgres"
   ```
4. Enable pgvector in Supabase SQL Editor:
   ```sql
   CREATE EXTENSION IF NOT EXISTS vector;
   ```
5. Apply Alembic migrations

---

### Task 3.3: Verification (Pending)

**Run verification script**:
```bash
poetry run python scripts/verify_database.py
```

**End-to-end pipeline test**:
```bash
poetry run python -c "
import asyncio
from ingestion.orchestrator import IngestionOrchestrator
from ingestion.embed import VectorStoreConfig
import os

async def test():
    orchestrator = IngestionOrchestrator(
        vector_store_config=VectorStoreConfig(
            collection_name='meditation_docs',
            db_url=os.getenv('DB_URL'),
        )
    )
    result = await orchestrator.process(
        source='https://example.com/article',
        title='Test Article'
    )
    print(f'Success: {result[\"success\"]}')
    print(f'Document ID: {result[\"document_id\"]}')

asyncio.run(test())
"
```

---

### Task 3.4: Documentation (Pending)

Files to create/update:
- [ ] `docs/deployment/SUPABASE_SETUP.md` - Complete Supabase setup guide
- [ ] `docs/deployment/ALEMBIC_GUIDE.md` - Migration management
- [ ] Update [CLAUDE.md](../../CLAUDE.md) with current status
- [ ] Update [README.md](../../README.md) with deployment instructions

---

## Code Metrics

### Phase 1: Code Refactoring
**Files Created**:
- `core/__init__.py`
- `core/exceptions.py`
- `core/interfaces.py`
- `ingestion/stages.py`

**Files Modified**:
- `ingestion/parsing.py` (major refactor)
- `ingestion/chunking.py` (thread-safe cache)
- `ingestion/orchestrator.py` (DAG architecture)
- `ingestion/__init__.py` (new exports)

**Lines of Code**:
- **Added**: ~800 lines (new modules)
- **Refactored**: ~400 lines (existing modules)
- **Total**: ~1200 lines

---

### Phase 2: Database Setup
**Files Created**:
- `scripts/verify_database.py` (170 lines)

**Files Modified**:
- `db/schema.py` - Added Chunk model, enhanced Document (+60 lines)
- `db/crud.py` - Removed EmbeddingCRUD, added ChunkCRUD (+180 lines)
- `config/settings.py` - Enhanced DatabaseSettings (+15 lines)
- `db/database.py` - Supabase detection and pooling (+40 lines)
- `ingestion/stages.py` - DatabasePersistenceStage (+105 lines)
- `ingestion/orchestrator.py` - Database integration (+50 lines)

**Lines of Code**:
- **Added**: ~450 lines (new functionality)
- **Refactored**: ~170 lines (existing modules)
- **Total**: ~620 lines

---

### Sprint 2 Total Impact
- **Files Created**: 5 (core modules + verification script)
- **Files Modified**: 10 (ingestion + database + config)
- **Total Lines**: ~1820 lines
- **Test Coverage**: Verification script with 7 checks

---

## Design Patterns Applied

1. **Strategy Pattern** - Parser selection
2. **Factory Pattern** - ParserFactory for parser creation
3. **Singleton Pattern** - ThreadSafeEmbeddingsCache
4. **Abstract Base Class** - PipelineStage
5. **Protocol (Structural Subtyping)** - Parser interface
6. **Immutable Data** - PipelineContext with `with_update()`
7. **DAG (Directed Acyclic Graph)** - Pipeline orchestration
8. **Dependency Injection** - Stages receive dependencies via constructor

---

## Backward Compatibility

âœ… **All existing code continues to work**:
- Old orchestrator API: `IngestionOrchestrator().process(source)`
- Old parsing functions: `html_to_markdown()`, `parse_pdf()` (with deprecation warnings)
- Old imports: `from ingestion import MarkdownChunker, VectorStoreManager`

---

## Next Steps

1. **Phase 2**: Update database schema and CRUD operations
2. **Phase 2**: Initialize Supabase with complete schema
3. **Phase 3**: Add DatabasePersistenceStage
4. **Phase 3**: Create comprehensive verification script
5. **Phase 3**: End-to-end testing with Supabase

---

## References

- **Plan**: [vast-weaving-moore.md](../../.claude/plans/vast-weaving-moore.md)
- **Roadmap**: [ROADMAP.md](../ROADMAP.md)
- **Main Context**: [CLAUDE.md](../../CLAUDE.md)
